{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DecipheringProteinLandscape_p450_colabversion_with_google_drive.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pgreisen/PEVAE_Paper/blob/master/DecipheringProteinLandscape_p450_colabversion_with_google_drive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvFNMIh4EsLb"
      },
      "source": [
        "Test the VAE model from the paper \"Deciphering protein evolution and fitness landscapes with latent space models\" "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmzH7KFxCb_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d02ac1-3a64-4c79-a875-99916c7ed39a"
      },
      "source": [
        "# mount google drive and copy files\n",
        "from google.colab import drive\n",
        "drive.mount('./drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at ./drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1cUp9-dC8JJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "422f5725-d16e-41df-be91-1d3357603230"
      },
      "source": [
        "!cp -r \"/content/drive/My Drive/Colab Notebooks/VAE/pfam_msa/script\" .\n",
        "!cp -r \"/content/drive/My Drive/Colab Notebooks/VAE/script\" ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/drive/My Drive/Colab Notebooks/VAE/pfam_msa/script': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh5HLU-ynrp9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f823633c-ba25-4f3e-d548-e4a532816595"
      },
      "source": [
        "# reinitialized working space\n",
        "!rm -r output/\n",
        "!rm -r MSA/\n",
        "!mkdir output/\n",
        "!mkdir output/model/\n",
        "!mkdir MSA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'output/': No such file or directory\n",
            "rm: cannot remove 'MSA/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcabNvBBCQXM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5ee0fe-6c9f-4ca5-b3a1-cde00e74c9e3"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQukyP2BnQ_a"
      },
      "source": [
        "import urllib3\n",
        "import gzip\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-w2WUo0nQ_c"
      },
      "source": [
        "# Insert pfam id\n",
        "# P450 // TAT\n",
        "pfam_id = 'PF00067'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GerYsVsbvDba"
      },
      "source": [
        "## Download the multiple sequence alignment for a given Pfam ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NdVMGtcnQ_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c988bb48-b820-4e52-9ed4-4c0ea9228d59"
      },
      "source": [
        "\n",
        "import urllib3\n",
        "import gzip\n",
        "import sys\n",
        "'''\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(description = \"Download the full multiple sequence alignment (MSA) in Stockholm format for a Pfam_id.\")\n",
        "parser.add_argument(\"--Pfam_id\", help = \"the ID of Pfam; e.g. PF00041\")\n",
        "args = parser.parse_args()\n",
        "pfam_id = args.Pfam_id'''\n",
        "\n",
        "\n",
        "print(\"Downloading the full multiple sequence alignment for Pfam: {0} ......\".format(pfam_id))\n",
        "http = urllib3.PoolManager()\n",
        "r = http.request('GET', 'http://pfam.xfam.org/family/{0}/alignment/full/gzipped'.format(pfam_id))\n",
        "data = gzip.decompress(r.data)\n",
        "data = data.decode()\n",
        "with open(\"./MSA/{0}_full.txt\".format(pfam_id), 'w') as file_handle:\n",
        "    print(data, file = file_handle)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading the full multiple sequence alignment for Pfam: PF00067 ......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0zfVdwb5sGr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrDRXOgtMEjx"
      },
      "source": [
        "# P450 A160 \n",
        "#gs = \"CP2CJ_HUMAN/30-487\"\n",
        "gs= \"D5DYE1_BACMQ/6-449\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_AN-HGWu6WS"
      },
      "source": [
        "## Process MSA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94frqtM94TS-"
      },
      "source": [
        "\n",
        "import pickle\n",
        "import sys\n",
        "import numpy as np\n",
        "from sys import exit\n",
        "\n",
        "\n",
        "pfam_name = pfam_id\n",
        "file_name = \"./MSA/\"+pfam_name+\"_full.txt\"\n",
        "query_seq_id = gs\n",
        "\n",
        "\n",
        "## read all the sequences into a dictionary\n",
        "seq_dict = {}\n",
        "with open(file_name, 'r') as file_handle:\n",
        "    for line in file_handle:\n",
        "        if line[0] == \"#\" or line[0] == \"/\" or line[0] == \"\":\n",
        "            continue\n",
        "        line = line.strip()\n",
        "        if len(line) == 0:\n",
        "            continue\n",
        "        seq_id, seq = line.split()\n",
        "\n",
        "        seq_dict[seq_id] = seq.upper()\n",
        "        \n",
        "## remove gaps in the query sequences\n",
        "query_seq = seq_dict[query_seq_id] ## with gaps\n",
        "idx = [ s == \"-\" or s == \".\" for s in query_seq]\n",
        "for k in seq_dict.keys():\n",
        "    seq_dict[k] = [seq_dict[k][i] for i in range(len(seq_dict[k])) if idx[i] == False]\n",
        "query_seq = seq_dict[query_seq_id] ## without gaps\n",
        "\n",
        "with open(\"./output/query_seq.fasta\", 'w') as file_handle:\n",
        "    print(\"> query\", file = file_handle)\n",
        "    print(\"\".join(query_seq), file = file_handle)\n",
        "\n",
        "## remove sequences with too many gaps\n",
        "len_query_seq = len(query_seq)\n",
        "seq_id = list(seq_dict.keys())\n",
        "num_gaps = []\n",
        "for k in seq_id:\n",
        "    num_gaps.append(seq_dict[k].count(\"-\") + seq_dict[k].count(\".\"))\n",
        "    #if seq_dict[k].count(\"-\") + seq_dict[k].count(\".\") > len_query_seq*0.10:\n",
        "    #print(\"len_query_seq*0.10 = %s\" % str(len_query_seq*0.10))\n",
        "    if seq_dict[k].count(\"-\") + seq_dict[k].count(\".\") > 30:\n",
        "        seq_dict.pop(k)\n",
        "\n",
        "with open(\"./output/seq_dict.pkl\", 'wb') as file_handle:\n",
        "    pickle.dump(seq_dict, file_handle)\n",
        "        \n",
        "## convert aa type into num 0-20\n",
        "aa = ['R', 'H', 'K',\n",
        "      'D', 'E',\n",
        "      'S', 'T', 'N', 'Q',\n",
        "      'C', 'G', 'P',\n",
        "      'A', 'V', 'I', 'L', 'M', 'F', 'Y', 'W']\n",
        "aa_index = {}\n",
        "aa_index['-'] = 0\n",
        "aa_index['.'] = 0\n",
        "i = 1\n",
        "for a in aa:\n",
        "    aa_index[a] = i\n",
        "    i += 1\n",
        "with open(\"./output/\" + \"/aa_index.pkl\", 'wb') as file_handle:\n",
        "    pickle.dump(aa_index, file_handle)\n",
        "    \n",
        "seq_msa = []\n",
        "keys_list = []\n",
        "for k in seq_dict.keys():\n",
        "    if seq_dict[k].count('X') > 0 or seq_dict[k].count('Z') > 0:\n",
        "        continue    \n",
        "    seq_msa.append([aa_index[s] for s in seq_dict[k]])\n",
        "    keys_list.append(k)    \n",
        "seq_msa = np.array(seq_msa)\n",
        "\n",
        "with open(\"./output/keys_list.pkl\", 'wb') as file_handle:\n",
        "    pickle.dump(keys_list, file_handle)\n",
        "\n",
        "## remove positions where too many sequences have gaps\n",
        "pos_idx = []\n",
        "for i in range(seq_msa.shape[1]):\n",
        "    if np.sum(seq_msa[:,i] == 0) <= seq_msa.shape[0]*0.20:\n",
        "    #if np.sum(seq_msa[:,i] == 0) <= seq_msa.shape[0]*0.20:\n",
        "        pos_idx.append(i)\n",
        "with open(\"./output/\" + \"/seq_pos_idx.pkl\", 'wb') as file_handle:\n",
        "    pickle.dump(pos_idx, file_handle)\n",
        "    \n",
        "seq_msa = seq_msa[:, np.array(pos_idx)]\n",
        "with open(\"./output/\" + \"/seq_msa.pkl\", 'wb') as file_handle:\n",
        "    pickle.dump(seq_msa, file_handle)\n",
        "\n",
        "## reweighting sequences\n",
        "seq_weight = np.zeros(seq_msa.shape)\n",
        "for j in range(seq_msa.shape[1]):\n",
        "    aa_type, aa_counts = np.unique(seq_msa[:,j], return_counts = True)\n",
        "    num_type = len(aa_type)\n",
        "    aa_dict = {}\n",
        "    for a in aa_type:\n",
        "        aa_dict[a] = aa_counts[list(aa_type).index(a)]\n",
        "    for i in range(seq_msa.shape[0]):\n",
        "        seq_weight[i,j] = (1.0/num_type) * (1.0/aa_dict[seq_msa[i,j]])\n",
        "tot_weight = np.sum(seq_weight)\n",
        "seq_weight = seq_weight.sum(1) / tot_weight \n",
        "with open(\"./output/\" + \"/seq_weight.pkl\", 'wb') as file_handle:\n",
        "    pickle.dump(seq_weight, file_handle)\n",
        "\n",
        "## change aa numbering into binary\n",
        "K = 21 ## num of classes of aa\n",
        "D = np.identity(K)\n",
        "num_seq = seq_msa.shape[0]\n",
        "len_seq_msa = seq_msa.shape[1]\n",
        "seq_msa_binary = np.zeros((num_seq, len_seq_msa, K))\n",
        "for i in range(num_seq):\n",
        "    seq_msa_binary[i,:,:] = D[seq_msa[i]]\n",
        "\n",
        "with open(\"./output/\" + \"/seq_msa_binary.pkl\", 'wb') as file_handle:\n",
        "    pickle.dump(seq_msa_binary, file_handle)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVA7xGUr8cBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c12247b-f803-488e-c731-de3349ef30c6"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "with open(\"./output/seq_msa.pkl\", 'rb') as file_handle:\n",
        "    seq_msa = pickle.load(file_handle)\n",
        "seq_msa = seq_msa.tolist()\n",
        "\n",
        "seq_msa_string = []\n",
        "for seq in seq_msa:    \n",
        "    seq_msa_string.append(\",\".join([str(s) for s in seq]))\n",
        "seq_msa_set = set(seq_msa_string)\n",
        "\n",
        "print(\"Original num of seq: {}\".format(len(seq_msa_string)))\n",
        "print(\"Num of uniq seq: {}\".format(len(seq_msa_set)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original num of seq: 41322\n",
            "Num of uniq seq: 38727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj0G6INXGDYT"
      },
      "source": [
        "## Training for the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2G0kKUqMwTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a4ee13-433e-4dd5-8f7f-4e4416532884"
      },
      "source": [
        "# train.py\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "# path needs to be set to your system\n",
        "sys.path.append(\"./script\")\n",
        "from VAE_model import *\n",
        "from sys import exit\n",
        "import argparse\n",
        "\n",
        "\n",
        "# PG edits\n",
        "num_epoches = 10000 # args.num_epoch\n",
        "weight_decay = 0.01 # args.weight_decay\n",
        "\n",
        "print(\"num_epoches: \", num_epoches)\n",
        "print(\"weight_decay: \", str(weight_decay))\n",
        "\n",
        "## read data\n",
        "with open(\"./output/seq_msa_binary.pkl\", 'rb') as file_handle:\n",
        "    seq_msa_binary = pickle.load(file_handle)    \n",
        "num_seq = seq_msa_binary.shape[0]\n",
        "len_protein = seq_msa_binary.shape[1]\n",
        "num_res_type = seq_msa_binary.shape[2]\n",
        "seq_msa_binary = seq_msa_binary.reshape((num_seq, -1))\n",
        "seq_msa_binary = seq_msa_binary.astype(np.float32)\n",
        "\n",
        "with open(\"./output/seq_weight.pkl\", 'rb') as file_handle:\n",
        "    seq_weight = pickle.load(file_handle)\n",
        "seq_weight = seq_weight.astype(np.float32)\n",
        "\n",
        "with open(\"./output/keys_list.pkl\", 'rb') as file_handle:\n",
        "    seq_keys = pickle.load(file_handle)\n",
        "\n",
        "#### training model with K-fold cross validation\n",
        "## split the data index 0:num_seq-1 into K sets\n",
        "## each set is just a set of indices of sequences.\n",
        "## in the kth traing, the kth subsets of sequences are used\n",
        "## as validation data and the remaining K-1 sets are used\n",
        "## as training data\n",
        "K = 5\n",
        "num_seq_subset = num_seq // K + 1\n",
        "idx_subset = []\n",
        "random_idx = np.random.permutation(range(num_seq))\n",
        "for i in range(K):\n",
        "    idx_subset.append(random_idx[i*num_seq_subset:(i+1)*num_seq_subset])\n",
        "\n",
        "## the following list holds the elbo values on validation data    \n",
        "elbo_all_list = []\n",
        "\n",
        "for k in range(K):\n",
        "    print(\"Start the {}th fold training\".format(k))\n",
        "    print(\"-\"*60)\n",
        "    \n",
        "    ## build a VAE model with random parameters\n",
        "    vae = VAE(21, 2, len_protein * num_res_type, [100])\n",
        "\n",
        "    ## move the VAE onto a GPU\n",
        "    vae.cuda()\n",
        "\n",
        "    ## build the Adam optimizer\n",
        "    optimizer = optim.Adam(vae.parameters(),\n",
        "                           weight_decay = weight_decay)\n",
        "\n",
        "    ## collect training and valiation data indices\n",
        "    validation_idx = idx_subset[k]\n",
        "    validation_idx.sort()\n",
        "    \n",
        "    train_idx = np.array(list(set(range(num_seq)) - set(validation_idx)))\n",
        "    train_idx.sort()\n",
        "\n",
        "    train_msa = torch.from_numpy(seq_msa_binary[train_idx, ])\n",
        "    train_msa = train_msa.cuda()\n",
        "\n",
        "    train_weight = torch.from_numpy(seq_weight[train_idx])\n",
        "    train_weight = train_weight/torch.sum(train_weight)\n",
        "    train_weight = train_weight.cuda()\n",
        "    \n",
        "    train_loss_list = []    \n",
        "    for epoch in range(num_epoches):    \n",
        "        loss = (-1)*vae.compute_weighted_elbo(train_msa, train_weight)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()        \n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss_list.append(loss.item())\n",
        "        if (epoch + 1) % 50 ==0:\n",
        "            print(\"Fold: {}, Epoch: {:>4}, loss: {:>4.2f}\".format(k, epoch, loss.item()), flush = True)\n",
        "\n",
        "    ## cope trained model to cpu and save it\n",
        "    vae.cpu()\n",
        "    torch.save(vae.state_dict(), \"./output/model/vae_{}_fold_{}.model\".format(str(weight_decay), k))\n",
        "\n",
        "    print(\"Finish the {}th fold training\".format(k))\n",
        "    print(\"=\"*60)\n",
        "    print('')\n",
        "    \n",
        "    print(\"Start the {}th fold validation\".format(k))\n",
        "    print(\"-\"*60)\n",
        "    ## evaluate the trained model \n",
        "    vae.cuda()\n",
        "    \n",
        "    elbo_on_validation_data_list = []\n",
        "    ## because the function vae.compute_elbo_with_multiple samples uses\n",
        "    ## a large amount of memory on GPUs. we have to split validation data\n",
        "    ## into batches.\n",
        "    batch_size = 32\n",
        "    num_batches = len(validation_idx)//batch_size + 1\n",
        "    for idx_batch in range(num_batches):\n",
        "        if (idx_batch + 1) % 50 == 0:\n",
        "            print(\"idx_batch: {} out of {}\".format(idx_batch, num_batches))        \n",
        "        validation_msa = seq_msa_binary[validation_idx[idx_batch*batch_size:(idx_batch+1)*batch_size]]\n",
        "        validation_msa = torch.from_numpy(validation_msa)\n",
        "        with torch.no_grad():\n",
        "            validation_msa = validation_msa.cuda()\n",
        "            elbo = vae.compute_elbo_with_multiple_samples(validation_msa, 5000)            \n",
        "            elbo_on_validation_data_list.append(elbo.cpu().data.numpy())\n",
        "\n",
        "    elbo_on_validation_data = np.concatenate(elbo_on_validation_data_list)    \n",
        "    elbo_all_list.append(elbo_on_validation_data)\n",
        "    \n",
        "    print(\"Finish the {}th fold validation\".format(k))\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "elbo_all = np.concatenate(elbo_all_list)\n",
        "elbo_mean = np.mean(elbo_all)\n",
        "## the mean_elbo can approximate the quanlity of the learned model\n",
        "## we want a model that has high mean_elbo\n",
        "## different weight decay factor or different network structure\n",
        "## will give different mean_elbo values and we want to choose the\n",
        "## weight decay factor or network structure that has large mean_elbo\n",
        "print(\"mean_elbo: {:.3f}\".format(elbo_mean))\n",
        "\n",
        "with open(\"./output/elbo_all.pkl\", 'wb') as file_handle:\n",
        "    pickle.dump(elbo_all, file_handle)\n",
        "\n",
        "## after choosing the weight decay and network structure that have\n",
        "## large mean_elbo, we can fix them and train a new model with all\n",
        "## the training data as before\n",
        "\n",
        "    \n",
        "#exit()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_epoches:  10000\n",
            "weight_decay:  0.01\n",
            "Start the 0th fold training\n",
            "------------------------------------------------------------\n",
            "Fold: 0, Epoch:   49, loss: 897.52\n",
            "Fold: 0, Epoch:   99, loss: 855.90\n",
            "Fold: 0, Epoch:  149, loss: 834.41\n",
            "Fold: 0, Epoch:  199, loss: 822.18\n",
            "Fold: 0, Epoch:  249, loss: 813.78\n",
            "Fold: 0, Epoch:  299, loss: 806.56\n",
            "Fold: 0, Epoch:  349, loss: 799.84\n",
            "Fold: 0, Epoch:  399, loss: 793.54\n",
            "Fold: 0, Epoch:  449, loss: 787.59\n",
            "Fold: 0, Epoch:  499, loss: 781.94\n",
            "Fold: 0, Epoch:  549, loss: 777.04\n",
            "Fold: 0, Epoch:  599, loss: 771.99\n",
            "Fold: 0, Epoch:  649, loss: 767.68\n",
            "Fold: 0, Epoch:  699, loss: 763.71\n",
            "Fold: 0, Epoch:  749, loss: 759.92\n",
            "Fold: 0, Epoch:  799, loss: 756.48\n",
            "Fold: 0, Epoch:  849, loss: 752.82\n",
            "Fold: 0, Epoch:  899, loss: 749.47\n",
            "Fold: 0, Epoch:  949, loss: 746.29\n",
            "Fold: 0, Epoch:  999, loss: 742.66\n",
            "Fold: 0, Epoch: 1049, loss: 739.50\n",
            "Fold: 0, Epoch: 1099, loss: 736.43\n",
            "Fold: 0, Epoch: 1149, loss: 733.60\n",
            "Fold: 0, Epoch: 1199, loss: 730.80\n",
            "Fold: 0, Epoch: 1249, loss: 728.27\n",
            "Fold: 0, Epoch: 1299, loss: 725.84\n",
            "Fold: 0, Epoch: 1349, loss: 723.71\n",
            "Fold: 0, Epoch: 1399, loss: 721.65\n",
            "Fold: 0, Epoch: 1449, loss: 719.74\n",
            "Fold: 0, Epoch: 1499, loss: 718.13\n",
            "Fold: 0, Epoch: 1549, loss: 716.07\n",
            "Fold: 0, Epoch: 1599, loss: 714.37\n",
            "Fold: 0, Epoch: 1649, loss: 712.69\n",
            "Fold: 0, Epoch: 1699, loss: 711.14\n",
            "Fold: 0, Epoch: 1749, loss: 709.48\n",
            "Fold: 0, Epoch: 1799, loss: 708.00\n",
            "Fold: 0, Epoch: 1849, loss: 706.55\n",
            "Fold: 0, Epoch: 1899, loss: 705.23\n",
            "Fold: 0, Epoch: 1949, loss: 703.97\n",
            "Fold: 0, Epoch: 1999, loss: 702.86\n",
            "Fold: 0, Epoch: 2049, loss: 701.61\n",
            "Fold: 0, Epoch: 2099, loss: 700.43\n",
            "Fold: 0, Epoch: 2149, loss: 699.42\n",
            "Fold: 0, Epoch: 2199, loss: 698.33\n",
            "Fold: 0, Epoch: 2249, loss: 697.55\n",
            "Fold: 0, Epoch: 2299, loss: 696.44\n",
            "Fold: 0, Epoch: 2349, loss: 695.52\n",
            "Fold: 0, Epoch: 2399, loss: 694.69\n",
            "Fold: 0, Epoch: 2449, loss: 693.77\n",
            "Fold: 0, Epoch: 2499, loss: 692.88\n",
            "Fold: 0, Epoch: 2549, loss: 691.99\n",
            "Fold: 0, Epoch: 2599, loss: 690.96\n",
            "Fold: 0, Epoch: 2649, loss: 690.08\n",
            "Fold: 0, Epoch: 2699, loss: 689.02\n",
            "Fold: 0, Epoch: 2749, loss: 688.14\n",
            "Fold: 0, Epoch: 2799, loss: 687.13\n",
            "Fold: 0, Epoch: 2849, loss: 686.35\n",
            "Fold: 0, Epoch: 2899, loss: 685.70\n",
            "Fold: 0, Epoch: 2949, loss: 684.83\n",
            "Fold: 0, Epoch: 2999, loss: 684.15\n",
            "Fold: 0, Epoch: 3049, loss: 683.50\n",
            "Fold: 0, Epoch: 3099, loss: 682.96\n",
            "Fold: 0, Epoch: 3149, loss: 682.31\n",
            "Fold: 0, Epoch: 3199, loss: 681.72\n",
            "Fold: 0, Epoch: 3249, loss: 681.20\n",
            "Fold: 0, Epoch: 3299, loss: 680.67\n",
            "Fold: 0, Epoch: 3349, loss: 680.56\n",
            "Fold: 0, Epoch: 3399, loss: 679.67\n",
            "Fold: 0, Epoch: 3449, loss: 679.07\n",
            "Fold: 0, Epoch: 3499, loss: 678.58\n",
            "Fold: 0, Epoch: 3549, loss: 678.24\n",
            "Fold: 0, Epoch: 3599, loss: 677.81\n",
            "Fold: 0, Epoch: 3649, loss: 677.25\n",
            "Fold: 0, Epoch: 3699, loss: 676.92\n",
            "Fold: 0, Epoch: 3749, loss: 676.36\n",
            "Fold: 0, Epoch: 3799, loss: 675.97\n",
            "Fold: 0, Epoch: 3849, loss: 675.62\n",
            "Fold: 0, Epoch: 3899, loss: 675.21\n",
            "Fold: 0, Epoch: 3949, loss: 675.16\n",
            "Fold: 0, Epoch: 3999, loss: 674.49\n",
            "Fold: 0, Epoch: 4049, loss: 674.18\n",
            "Fold: 0, Epoch: 4099, loss: 673.82\n",
            "Fold: 0, Epoch: 4149, loss: 673.52\n",
            "Fold: 0, Epoch: 4199, loss: 673.27\n",
            "Fold: 0, Epoch: 4249, loss: 672.93\n",
            "Fold: 0, Epoch: 4299, loss: 672.59\n",
            "Fold: 0, Epoch: 4349, loss: 672.31\n",
            "Fold: 0, Epoch: 4399, loss: 671.98\n",
            "Fold: 0, Epoch: 4449, loss: 671.73\n",
            "Fold: 0, Epoch: 4499, loss: 671.45\n",
            "Fold: 0, Epoch: 4549, loss: 671.27\n",
            "Fold: 0, Epoch: 4599, loss: 671.00\n",
            "Fold: 0, Epoch: 4649, loss: 670.70\n",
            "Fold: 0, Epoch: 4699, loss: 670.34\n",
            "Fold: 0, Epoch: 4749, loss: 670.13\n",
            "Fold: 0, Epoch: 4799, loss: 669.83\n",
            "Fold: 0, Epoch: 4849, loss: 669.86\n",
            "Fold: 0, Epoch: 4899, loss: 669.31\n",
            "Fold: 0, Epoch: 4949, loss: 669.08\n",
            "Fold: 0, Epoch: 4999, loss: 669.38\n",
            "Fold: 0, Epoch: 5049, loss: 668.54\n",
            "Fold: 0, Epoch: 5099, loss: 668.29\n",
            "Fold: 0, Epoch: 5149, loss: 668.06\n",
            "Fold: 0, Epoch: 5199, loss: 667.84\n",
            "Fold: 0, Epoch: 5249, loss: 667.58\n",
            "Fold: 0, Epoch: 5299, loss: 667.34\n",
            "Fold: 0, Epoch: 5349, loss: 667.25\n",
            "Fold: 0, Epoch: 5399, loss: 666.92\n",
            "Fold: 0, Epoch: 5449, loss: 666.69\n",
            "Fold: 0, Epoch: 5499, loss: 666.49\n",
            "Fold: 0, Epoch: 5549, loss: 666.35\n",
            "Fold: 0, Epoch: 5599, loss: 666.15\n",
            "Fold: 0, Epoch: 5649, loss: 666.36\n",
            "Fold: 0, Epoch: 5699, loss: 665.79\n",
            "Fold: 0, Epoch: 5749, loss: 665.64\n",
            "Fold: 0, Epoch: 5799, loss: 665.48\n",
            "Fold: 0, Epoch: 5849, loss: 665.31\n",
            "Fold: 0, Epoch: 5899, loss: 665.09\n",
            "Fold: 0, Epoch: 5949, loss: 664.95\n",
            "Fold: 0, Epoch: 5999, loss: 664.79\n",
            "Fold: 0, Epoch: 6049, loss: 664.65\n",
            "Fold: 0, Epoch: 6099, loss: 664.48\n",
            "Fold: 0, Epoch: 6149, loss: 665.12\n",
            "Fold: 0, Epoch: 6199, loss: 664.21\n",
            "Fold: 0, Epoch: 6249, loss: 664.10\n",
            "Fold: 0, Epoch: 6299, loss: 664.01\n",
            "Fold: 0, Epoch: 6349, loss: 663.78\n",
            "Fold: 0, Epoch: 6399, loss: 663.61\n",
            "Fold: 0, Epoch: 6449, loss: 663.49\n",
            "Fold: 0, Epoch: 6499, loss: 663.44\n",
            "Fold: 0, Epoch: 6549, loss: 663.28\n",
            "Fold: 0, Epoch: 6599, loss: 663.20\n",
            "Fold: 0, Epoch: 6649, loss: 663.03\n",
            "Fold: 0, Epoch: 6699, loss: 662.93\n",
            "Fold: 0, Epoch: 6749, loss: 662.95\n",
            "Fold: 0, Epoch: 6799, loss: 662.70\n",
            "Fold: 0, Epoch: 6849, loss: 662.54\n",
            "Fold: 0, Epoch: 6899, loss: 662.44\n",
            "Fold: 0, Epoch: 6949, loss: 663.36\n",
            "Fold: 0, Epoch: 6999, loss: 662.14\n",
            "Fold: 0, Epoch: 7049, loss: 662.06\n",
            "Fold: 0, Epoch: 7099, loss: 661.90\n",
            "Fold: 0, Epoch: 7149, loss: 661.80\n",
            "Fold: 0, Epoch: 7199, loss: 661.68\n",
            "Fold: 0, Epoch: 7249, loss: 661.83\n",
            "Fold: 0, Epoch: 7299, loss: 661.53\n",
            "Fold: 0, Epoch: 7349, loss: 661.85\n",
            "Fold: 0, Epoch: 7399, loss: 661.27\n",
            "Fold: 0, Epoch: 7449, loss: 661.18\n",
            "Fold: 0, Epoch: 7499, loss: 661.30\n",
            "Fold: 0, Epoch: 7549, loss: 661.02\n",
            "Fold: 0, Epoch: 7599, loss: 660.97\n",
            "Fold: 0, Epoch: 7649, loss: 660.84\n",
            "Fold: 0, Epoch: 7699, loss: 660.79\n",
            "Fold: 0, Epoch: 7749, loss: 660.65\n",
            "Fold: 0, Epoch: 7799, loss: 660.59\n",
            "Fold: 0, Epoch: 7849, loss: 660.55\n",
            "Fold: 0, Epoch: 7899, loss: 660.45\n",
            "Fold: 0, Epoch: 7949, loss: 660.32\n",
            "Fold: 0, Epoch: 7999, loss: 660.28\n",
            "Fold: 0, Epoch: 8049, loss: 660.24\n",
            "Fold: 0, Epoch: 8099, loss: 660.17\n",
            "Fold: 0, Epoch: 8149, loss: 660.08\n",
            "Fold: 0, Epoch: 8199, loss: 660.10\n",
            "Fold: 0, Epoch: 8249, loss: 660.00\n",
            "Fold: 0, Epoch: 8299, loss: 660.72\n",
            "Fold: 0, Epoch: 8349, loss: 659.84\n",
            "Fold: 0, Epoch: 8399, loss: 659.84\n",
            "Fold: 0, Epoch: 8449, loss: 659.85\n",
            "Fold: 0, Epoch: 8499, loss: 659.77\n",
            "Fold: 0, Epoch: 8549, loss: 659.68\n",
            "Fold: 0, Epoch: 8599, loss: 659.65\n",
            "Fold: 0, Epoch: 8649, loss: 659.75\n",
            "Fold: 0, Epoch: 8699, loss: 659.54\n",
            "Fold: 0, Epoch: 8749, loss: 659.55\n",
            "Fold: 0, Epoch: 8799, loss: 659.45\n",
            "Fold: 0, Epoch: 8849, loss: 659.44\n",
            "Fold: 0, Epoch: 8899, loss: 659.37\n",
            "Fold: 0, Epoch: 8949, loss: 659.40\n",
            "Fold: 0, Epoch: 8999, loss: 659.60\n",
            "Fold: 0, Epoch: 9049, loss: 659.22\n",
            "Fold: 0, Epoch: 9099, loss: 659.16\n",
            "Fold: 0, Epoch: 9149, loss: 659.21\n",
            "Fold: 0, Epoch: 9199, loss: 659.02\n",
            "Fold: 0, Epoch: 9249, loss: 659.02\n",
            "Fold: 0, Epoch: 9299, loss: 658.96\n",
            "Fold: 0, Epoch: 9349, loss: 659.04\n",
            "Fold: 0, Epoch: 9399, loss: 658.91\n",
            "Fold: 0, Epoch: 9449, loss: 659.06\n",
            "Fold: 0, Epoch: 9499, loss: 658.89\n",
            "Fold: 0, Epoch: 9549, loss: 658.93\n",
            "Fold: 0, Epoch: 9599, loss: 658.84\n",
            "Fold: 0, Epoch: 9649, loss: 658.86\n",
            "Fold: 0, Epoch: 9699, loss: 658.88\n",
            "Fold: 0, Epoch: 9749, loss: 658.78\n",
            "Fold: 0, Epoch: 9799, loss: 658.75\n",
            "Fold: 0, Epoch: 9849, loss: 658.78\n",
            "Fold: 0, Epoch: 9899, loss: 658.68\n",
            "Fold: 0, Epoch: 9949, loss: 658.62\n",
            "Fold: 0, Epoch: 9999, loss: 658.59\n",
            "Finish the 0th fold training\n",
            "============================================================\n",
            "\n",
            "Start the 0th fold validation\n",
            "------------------------------------------------------------\n",
            "idx_batch: 49 out of 259\n",
            "idx_batch: 99 out of 259\n",
            "idx_batch: 149 out of 259\n",
            "idx_batch: 199 out of 259\n",
            "idx_batch: 249 out of 259\n",
            "Finish the 0th fold validation\n",
            "============================================================\n",
            "Start the 1th fold training\n",
            "------------------------------------------------------------\n",
            "Fold: 1, Epoch:   49, loss: 886.23\n",
            "Fold: 1, Epoch:   99, loss: 843.24\n",
            "Fold: 1, Epoch:  149, loss: 829.25\n",
            "Fold: 1, Epoch:  199, loss: 819.31\n",
            "Fold: 1, Epoch:  249, loss: 811.48\n",
            "Fold: 1, Epoch:  299, loss: 804.42\n",
            "Fold: 1, Epoch:  349, loss: 797.54\n",
            "Fold: 1, Epoch:  399, loss: 790.65\n",
            "Fold: 1, Epoch:  449, loss: 783.80\n",
            "Fold: 1, Epoch:  499, loss: 777.30\n",
            "Fold: 1, Epoch:  549, loss: 771.65\n",
            "Fold: 1, Epoch:  599, loss: 766.69\n",
            "Fold: 1, Epoch:  649, loss: 762.23\n",
            "Fold: 1, Epoch:  699, loss: 758.15\n",
            "Fold: 1, Epoch:  749, loss: 754.19\n",
            "Fold: 1, Epoch:  799, loss: 750.48\n",
            "Fold: 1, Epoch:  849, loss: 746.34\n",
            "Fold: 1, Epoch:  899, loss: 742.49\n",
            "Fold: 1, Epoch:  949, loss: 738.85\n",
            "Fold: 1, Epoch:  999, loss: 735.44\n",
            "Fold: 1, Epoch: 1049, loss: 732.25\n",
            "Fold: 1, Epoch: 1099, loss: 729.26\n",
            "Fold: 1, Epoch: 1149, loss: 726.48\n",
            "Fold: 1, Epoch: 1199, loss: 723.92\n",
            "Fold: 1, Epoch: 1249, loss: 721.68\n",
            "Fold: 1, Epoch: 1299, loss: 719.55\n",
            "Fold: 1, Epoch: 1349, loss: 717.65\n",
            "Fold: 1, Epoch: 1399, loss: 715.95\n",
            "Fold: 1, Epoch: 1449, loss: 714.34\n",
            "Fold: 1, Epoch: 1499, loss: 712.90\n",
            "Fold: 1, Epoch: 1549, loss: 711.42\n",
            "Fold: 1, Epoch: 1599, loss: 710.13\n",
            "Fold: 1, Epoch: 1649, loss: 708.79\n",
            "Fold: 1, Epoch: 1699, loss: 707.36\n",
            "Fold: 1, Epoch: 1749, loss: 706.09\n",
            "Fold: 1, Epoch: 1799, loss: 704.89\n",
            "Fold: 1, Epoch: 1849, loss: 703.87\n",
            "Fold: 1, Epoch: 1899, loss: 702.65\n",
            "Fold: 1, Epoch: 1949, loss: 701.51\n",
            "Fold: 1, Epoch: 1999, loss: 700.44\n",
            "Fold: 1, Epoch: 2049, loss: 699.44\n",
            "Fold: 1, Epoch: 2099, loss: 698.51\n",
            "Fold: 1, Epoch: 2149, loss: 698.02\n",
            "Fold: 1, Epoch: 2199, loss: 696.70\n",
            "Fold: 1, Epoch: 2249, loss: 695.82\n",
            "Fold: 1, Epoch: 2299, loss: 694.92\n",
            "Fold: 1, Epoch: 2349, loss: 694.06\n",
            "Fold: 1, Epoch: 2399, loss: 693.28\n",
            "Fold: 1, Epoch: 2449, loss: 692.46\n",
            "Fold: 1, Epoch: 2499, loss: 691.76\n",
            "Fold: 1, Epoch: 2549, loss: 690.97\n",
            "Fold: 1, Epoch: 2599, loss: 690.25\n",
            "Fold: 1, Epoch: 2649, loss: 689.50\n",
            "Fold: 1, Epoch: 2699, loss: 688.81\n",
            "Fold: 1, Epoch: 2749, loss: 688.04\n",
            "Fold: 1, Epoch: 2799, loss: 687.37\n",
            "Fold: 1, Epoch: 2849, loss: 686.79\n",
            "Fold: 1, Epoch: 2899, loss: 686.03\n",
            "Fold: 1, Epoch: 2949, loss: 685.38\n",
            "Fold: 1, Epoch: 2999, loss: 684.82\n",
            "Fold: 1, Epoch: 3049, loss: 684.16\n",
            "Fold: 1, Epoch: 3099, loss: 683.77\n",
            "Fold: 1, Epoch: 3149, loss: 682.76\n",
            "Fold: 1, Epoch: 3199, loss: 682.16\n",
            "Fold: 1, Epoch: 3249, loss: 681.53\n",
            "Fold: 1, Epoch: 3299, loss: 680.94\n",
            "Fold: 1, Epoch: 3349, loss: 680.42\n",
            "Fold: 1, Epoch: 3399, loss: 679.79\n",
            "Fold: 1, Epoch: 3449, loss: 679.28\n",
            "Fold: 1, Epoch: 3499, loss: 678.90\n",
            "Fold: 1, Epoch: 3549, loss: 678.40\n",
            "Fold: 1, Epoch: 3599, loss: 677.81\n",
            "Fold: 1, Epoch: 3649, loss: 677.29\n",
            "Fold: 1, Epoch: 3699, loss: 676.84\n",
            "Fold: 1, Epoch: 3749, loss: 676.44\n",
            "Fold: 1, Epoch: 3799, loss: 676.04\n",
            "Fold: 1, Epoch: 3849, loss: 675.60\n",
            "Fold: 1, Epoch: 3899, loss: 675.26\n",
            "Fold: 1, Epoch: 3949, loss: 674.83\n",
            "Fold: 1, Epoch: 3999, loss: 674.61\n",
            "Fold: 1, Epoch: 4049, loss: 674.10\n",
            "Fold: 1, Epoch: 4099, loss: 673.84\n",
            "Fold: 1, Epoch: 4149, loss: 673.36\n",
            "Fold: 1, Epoch: 4199, loss: 673.06\n",
            "Fold: 1, Epoch: 4249, loss: 672.74\n",
            "Fold: 1, Epoch: 4299, loss: 672.51\n",
            "Fold: 1, Epoch: 4349, loss: 671.85\n",
            "Fold: 1, Epoch: 4399, loss: 671.53\n",
            "Fold: 1, Epoch: 4449, loss: 671.27\n",
            "Fold: 1, Epoch: 4499, loss: 670.78\n",
            "Fold: 1, Epoch: 4549, loss: 670.45\n",
            "Fold: 1, Epoch: 4599, loss: 670.30\n",
            "Fold: 1, Epoch: 4649, loss: 669.84\n",
            "Fold: 1, Epoch: 4699, loss: 669.57\n",
            "Fold: 1, Epoch: 4749, loss: 669.31\n",
            "Fold: 1, Epoch: 4799, loss: 669.00\n",
            "Fold: 1, Epoch: 4849, loss: 668.74\n",
            "Fold: 1, Epoch: 4899, loss: 668.46\n",
            "Fold: 1, Epoch: 4949, loss: 668.39\n",
            "Fold: 1, Epoch: 4999, loss: 667.98\n",
            "Fold: 1, Epoch: 5049, loss: 667.71\n",
            "Fold: 1, Epoch: 5099, loss: 667.48\n",
            "Fold: 1, Epoch: 5149, loss: 667.35\n",
            "Fold: 1, Epoch: 5199, loss: 667.00\n",
            "Fold: 1, Epoch: 5249, loss: 666.83\n",
            "Fold: 1, Epoch: 5299, loss: 666.54\n",
            "Fold: 1, Epoch: 5349, loss: 666.25\n",
            "Fold: 1, Epoch: 5399, loss: 666.04\n",
            "Fold: 1, Epoch: 5449, loss: 665.85\n",
            "Fold: 1, Epoch: 5499, loss: 665.62\n",
            "Fold: 1, Epoch: 5549, loss: 665.52\n",
            "Fold: 1, Epoch: 5599, loss: 665.20\n",
            "Fold: 1, Epoch: 5649, loss: 665.27\n",
            "Fold: 1, Epoch: 5699, loss: 664.82\n",
            "Fold: 1, Epoch: 5749, loss: 664.60\n",
            "Fold: 1, Epoch: 5799, loss: 664.41\n",
            "Fold: 1, Epoch: 5849, loss: 664.20\n",
            "Fold: 1, Epoch: 5899, loss: 663.99\n",
            "Fold: 1, Epoch: 5949, loss: 664.31\n",
            "Fold: 1, Epoch: 5999, loss: 663.62\n",
            "Fold: 1, Epoch: 6049, loss: 663.44\n",
            "Fold: 1, Epoch: 6099, loss: 663.26\n",
            "Fold: 1, Epoch: 6149, loss: 663.08\n",
            "Fold: 1, Epoch: 6199, loss: 662.96\n",
            "Fold: 1, Epoch: 6249, loss: 663.86\n",
            "Fold: 1, Epoch: 6299, loss: 662.67\n",
            "Fold: 1, Epoch: 6349, loss: 662.49\n",
            "Fold: 1, Epoch: 6399, loss: 662.34\n",
            "Fold: 1, Epoch: 6449, loss: 662.21\n",
            "Fold: 1, Epoch: 6499, loss: 662.04\n",
            "Fold: 1, Epoch: 6549, loss: 661.88\n",
            "Fold: 1, Epoch: 6599, loss: 661.71\n",
            "Fold: 1, Epoch: 6649, loss: 661.62\n",
            "Fold: 1, Epoch: 6699, loss: 661.45\n",
            "Fold: 1, Epoch: 6749, loss: 661.31\n",
            "Fold: 1, Epoch: 6799, loss: 661.15\n",
            "Fold: 1, Epoch: 6849, loss: 661.05\n",
            "Fold: 1, Epoch: 6899, loss: 661.24\n",
            "Fold: 1, Epoch: 6949, loss: 660.73\n",
            "Fold: 1, Epoch: 6999, loss: 660.67\n",
            "Fold: 1, Epoch: 7049, loss: 660.43\n",
            "Fold: 1, Epoch: 7099, loss: 660.24\n",
            "Fold: 1, Epoch: 7149, loss: 660.09\n",
            "Fold: 1, Epoch: 7199, loss: 660.33\n",
            "Fold: 1, Epoch: 7249, loss: 659.81\n",
            "Fold: 1, Epoch: 7299, loss: 659.70\n",
            "Fold: 1, Epoch: 7349, loss: 659.76\n",
            "Fold: 1, Epoch: 7399, loss: 659.46\n",
            "Fold: 1, Epoch: 7449, loss: 659.46\n",
            "Fold: 1, Epoch: 7499, loss: 659.44\n",
            "Fold: 1, Epoch: 7549, loss: 659.31\n",
            "Fold: 1, Epoch: 7599, loss: 659.14\n",
            "Fold: 1, Epoch: 7649, loss: 659.08\n",
            "Fold: 1, Epoch: 7699, loss: 658.97\n",
            "Fold: 1, Epoch: 7749, loss: 658.99\n",
            "Fold: 1, Epoch: 7799, loss: 658.78\n",
            "Fold: 1, Epoch: 7849, loss: 658.73\n",
            "Fold: 1, Epoch: 7899, loss: 658.63\n",
            "Fold: 1, Epoch: 7949, loss: 658.62\n",
            "Fold: 1, Epoch: 7999, loss: 658.54\n",
            "Fold: 1, Epoch: 8049, loss: 658.49\n",
            "Fold: 1, Epoch: 8099, loss: 658.35\n",
            "Fold: 1, Epoch: 8149, loss: 658.28\n",
            "Fold: 1, Epoch: 8199, loss: 658.20\n",
            "Fold: 1, Epoch: 8249, loss: 658.34\n",
            "Fold: 1, Epoch: 8299, loss: 658.11\n",
            "Fold: 1, Epoch: 8349, loss: 657.99\n",
            "Fold: 1, Epoch: 8399, loss: 657.96\n",
            "Fold: 1, Epoch: 8449, loss: 657.92\n",
            "Fold: 1, Epoch: 8499, loss: 657.90\n",
            "Fold: 1, Epoch: 8549, loss: 658.46\n",
            "Fold: 1, Epoch: 8599, loss: 657.76\n",
            "Fold: 1, Epoch: 8649, loss: 657.68\n",
            "Fold: 1, Epoch: 8699, loss: 657.62\n",
            "Fold: 1, Epoch: 8749, loss: 657.65\n",
            "Fold: 1, Epoch: 8799, loss: 657.56\n",
            "Fold: 1, Epoch: 8849, loss: 657.59\n",
            "Fold: 1, Epoch: 8899, loss: 657.46\n",
            "Fold: 1, Epoch: 8949, loss: 657.52\n",
            "Fold: 1, Epoch: 8999, loss: 657.40\n",
            "Fold: 1, Epoch: 9049, loss: 657.64\n",
            "Fold: 1, Epoch: 9099, loss: 657.31\n",
            "Fold: 1, Epoch: 9149, loss: 657.26\n",
            "Fold: 1, Epoch: 9199, loss: 657.25\n",
            "Fold: 1, Epoch: 9249, loss: 657.20\n",
            "Fold: 1, Epoch: 9299, loss: 657.07\n",
            "Fold: 1, Epoch: 9349, loss: 657.16\n",
            "Fold: 1, Epoch: 9399, loss: 657.00\n",
            "Fold: 1, Epoch: 9449, loss: 656.98\n",
            "Fold: 1, Epoch: 9499, loss: 656.99\n",
            "Fold: 1, Epoch: 9549, loss: 656.88\n",
            "Fold: 1, Epoch: 9599, loss: 656.96\n",
            "Fold: 1, Epoch: 9649, loss: 656.85\n",
            "Fold: 1, Epoch: 9699, loss: 656.76\n",
            "Fold: 1, Epoch: 9749, loss: 656.75\n",
            "Fold: 1, Epoch: 9799, loss: 656.78\n",
            "Fold: 1, Epoch: 9849, loss: 656.73\n",
            "Fold: 1, Epoch: 9899, loss: 656.81\n",
            "Fold: 1, Epoch: 9949, loss: 656.60\n",
            "Fold: 1, Epoch: 9999, loss: 656.60\n",
            "Finish the 1th fold training\n",
            "============================================================\n",
            "\n",
            "Start the 1th fold validation\n",
            "------------------------------------------------------------\n",
            "idx_batch: 49 out of 259\n",
            "idx_batch: 99 out of 259\n",
            "idx_batch: 149 out of 259\n",
            "idx_batch: 199 out of 259\n",
            "idx_batch: 249 out of 259\n",
            "Finish the 1th fold validation\n",
            "============================================================\n",
            "Start the 2th fold training\n",
            "------------------------------------------------------------\n",
            "Fold: 2, Epoch:   49, loss: 882.77\n",
            "Fold: 2, Epoch:   99, loss: 846.36\n",
            "Fold: 2, Epoch:  149, loss: 829.99\n",
            "Fold: 2, Epoch:  199, loss: 815.64\n",
            "Fold: 2, Epoch:  249, loss: 805.79\n",
            "Fold: 2, Epoch:  299, loss: 798.71\n",
            "Fold: 2, Epoch:  349, loss: 792.65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09ffMmBph8xH"
      },
      "source": [
        "Plotting the results from the VAE of the immunoglobolin family"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4TJnIdqvRvC"
      },
      "source": [
        "## Analysis the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNpkRZs0iA6X"
      },
      "source": [
        "\n",
        "\n",
        "import matplotlib as mpl\n",
        "mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "import pandas\n",
        "# PG edits\n",
        "#from model import *\n",
        "import sys\n",
        "sys.path.append(\"/content/script\")\n",
        "from VAE_model import *\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sys import exit\n",
        "\n",
        "mpl.rc('font', size = 16)\n",
        "mpl.rc('axes', titlesize = 'large', labelsize = 'large')\n",
        "mpl.rc('xtick', labelsize = 'large')\n",
        "mpl.rc('ytick', labelsize = 'large')\n",
        "\n",
        "## read data\n",
        "with open(\"./output/seq_msa_binary.pkl\", 'rb') as file_handle:\n",
        "    seq_msa_binary = pickle.load(file_handle)    \n",
        "num_seq = seq_msa_binary.shape[0]\n",
        "len_protein = seq_msa_binary.shape[1]\n",
        "num_res_type = seq_msa_binary.shape[2]\n",
        "seq_msa_binary = seq_msa_binary.reshape((num_seq, -1))\n",
        "seq_msa_binary = seq_msa_binary.astype(np.float32)\n",
        "\n",
        "with open(\"./output/seq_weight.pkl\", 'rb') as file_handle:\n",
        "    seq_weight = pickle.load(file_handle)\n",
        "seq_weight = seq_weight.astype(np.float32)\n",
        "\n",
        "batch_size = num_seq\n",
        "# PG comments\n",
        "#train_data = MSA_Dataset(seq_msa_binary, seq_weight)\n",
        "train_data = MSA_Dataset(seq_msa_binary, seq_weight, seq_keys)\n",
        "train_data_loader = DataLoader(train_data, batch_size = batch_size)\n",
        "VAE(21, 2, len_protein * num_res_type, [100])\n",
        "vae.cuda()\n",
        "vae.load_state_dict(torch.load(\"./output/model/vae_0.01_fold_0.model\"))\n",
        "\n",
        "for idx, data in enumerate(train_data_loader):\n",
        "    # PG comment\n",
        "    #msa, weight = data\n",
        "    msa, weight = data[0],data[1]\n",
        "    with torch.no_grad():\n",
        "        msa = Variable(msa).cuda()   \n",
        "        mu, sigma, p = vae.forward(msa)\n",
        "\n",
        "mu = mu.cpu().data.numpy()\n",
        "sigma = sigma.cpu().data.numpy()\n",
        "p = p.cpu().data.numpy()\n",
        "with open(\"./output/latent_space.pkl\", 'wb') as file_handle:\n",
        "    pickle.dump({'mu':mu, 'sigma': sigma, 'p': p}, file_handle)\n",
        "\n",
        "with open(\"./output/latent_space.pkl\", 'rb') as file_handle:\n",
        "    latent_space = pickle.load(file_handle)\n",
        "mu = latent_space['mu']\n",
        "sigma = latent_space['sigma']\n",
        "p = latent_space['p']\n",
        "    \n",
        "plt.figure(0)\n",
        "plt.clf()\n",
        "plt.plot(mu[:,0], mu[:,1], '.', alpha = 0.1, markersize = 3)\n",
        "plt.xlim((-6,6))\n",
        "plt.ylim((-6,6))\n",
        "plt.xlabel(\"$Z_1$\")\n",
        "plt.ylabel(\"$Z_2$\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"./output/immuno_globolins_latent_mu_scatter.png\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lapBve_PvX5t"
      },
      "source": [
        "## project latent space to sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIBgCF8Hq7-V"
      },
      "source": [
        "\n",
        "\n",
        "import numpy\n",
        "import torch\n",
        "import sys\n",
        "sys.path.append(\"/content/script\")\n",
        "from VAE_model import *\n",
        "import pickle\n",
        "import argparse\n",
        "from sys import exit\n",
        "\n",
        "## read training to get the length of protein and the number of\n",
        "## amino acide types\n",
        "with open(\"./output/seq_msa_binary.pkl\", 'rb') as file_handle:\n",
        "    seq_msa_binary = pickle.load(file_handle)\n",
        "len_protein = seq_msa_binary.shape[1]\n",
        "num_aa_type = seq_msa_binary.shape[2]\n",
        "\n",
        "## load trained VAE model    \n",
        "vae = VAE(21, 2, len_protein * num_aa_type, [100])\n",
        "model_state_dict = torch.load(\"./output/model/vae_0.01_fold_0.model\")\n",
        "vae.load_state_dict(model_state_dict)\n",
        "\n",
        "## define points in latent space to be converted into sequences\n",
        "## here I will just use random points as an example\n",
        "num_seqs = 5\n",
        "dim_latent_space = 2\n",
        "z = torch.randn(num_seqs, dim_latent_space)\n",
        "## z has to be a torch.tensor with a size of (num_seqs, dim_latent_space)\n",
        "with torch.no_grad():\n",
        "    log_p = vae.decoder(z)\n",
        "    p = torch.exp(log_p)\n",
        "    p = torch.reshape(p, (num_seqs, len_protein, num_aa_type))\n",
        "p = p.numpy()\n",
        "\n",
        "## p is a numpy array with shape [num_seqs, len_protein, num_aa_type].\n",
        "## p[i,:,:] represents the sequence converted from the latent space point z[i,:].\n",
        "## p[i,:,:] represents the probabilities of each amino acid at all positions of the i'th sequence.\n",
        "## Therefore, each point in the latent space is converted into a distribution of protein sequences.\n",
        "## This distribution is defined by probabilities of amino acid types at all positions.\n",
        "## p[i,j,:] is a vector of 21 probablities represent the probablity of 21 amino acid type\n",
        "## at the j'th position of the i'th sequence, so np.sum(p[i,j,:]) = 1\n",
        "assert(np.all(np.sum(p, -1) - 1 <= 1e-6))\n",
        "\n",
        "## the mapping between positions and amino acid types are stored in aa_index\n",
        "with open(\"./output/aa_index.pkl\", 'rb') as file_handle:\n",
        "    aa_index = pickle.load(file_handle)\n",
        "idx_to_aa_dict = {}\n",
        "idx_to_aa_list = ['' for i in range(num_aa_type)]\n",
        "for k, v in aa_index.items():\n",
        "    idx_to_aa_dict[v] = k\n",
        "    idx_to_aa_list[v] = k\n",
        "\n",
        "## we can convert probablities into actual sequences by choosing the most likely sequences.\n",
        "max_prob_idx = np.argmax(p, -1)\n",
        "seqs = []\n",
        "for i in range(num_seqs):\n",
        "    seq = [idx_to_aa_list[idx] for idx in max_prob_idx[i,:]]\n",
        "    seqs.append(\"\".join(seq))\n",
        "\n",
        "## the converted most likely sequences are in seqs\n",
        "'''\n",
        "for i in range(num_seqs):\n",
        "    print(seqs[i])\n",
        "    '''\n",
        "\n",
        "\n",
        "## we can also get the second, third most likely sequences from p for each point in the latent space.\n",
        "\n",
        "## YYY edited\n",
        "print(len(seqs))\n",
        "# save projected sequences\n",
        "with open('./output/projected_sequence.fasta','w') as seq_handle:\n",
        "    seq_handle.write(''.join(['>%s\\n%s\\n'%(x,y) for x,y in zip(range(len(seqs)),seqs)]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MA7I50xinZ_"
      },
      "source": [
        "## make_chimera_sequences.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFk_h6SrimZ8"
      },
      "source": [
        "__author__ = \"Xinqiang Ding <xqding@umich.edu>\"\n",
        "__date__ = \"2018/06/07 20:22:03\"\n",
        "\n",
        "import numpy as np\n",
        "from sys import exit\n",
        "import pickle\n",
        "\n",
        "## parent sequences of CYP102A1, CYP102A2, CYP102A3\n",
        "parent_sequences = []\n",
        "with open(\"./chimera_sequence/parent_sequences.txt\", 'r') as file_handle:\n",
        "    for line in file_handle:\n",
        "        line = line.strip()\n",
        "        fields = line.split()\n",
        "        parent_sequences.append(fields[1])\n",
        "\n",
        "## each parent sequence is seperated into 8 segments        \n",
        "positions = [64, 122, 166, 216, 268, 328, 404]\n",
        "align_positions = [] ## correspondings position in the alignment\n",
        "k = 0\n",
        "for i in range(len(parent_sequences[0])):\n",
        "    if parent_sequences[0][i] != \".\":\n",
        "        k += 1\n",
        "    if k in positions:\n",
        "        align_positions.append(i)\n",
        "        \n",
        "## cut parent sequences into 8 segments\n",
        "segments = []\n",
        "for k in range(3):\n",
        "    segments.append([])\n",
        "    start = 0\n",
        "    end = align_positions[0]+1\n",
        "    for i in range(0, len(align_positions)+1):\n",
        "        if i == 0:\n",
        "            start = 0\n",
        "        else:\n",
        "            start = align_positions[i-1] + 1\n",
        "\n",
        "        if i >= len(align_positions):\n",
        "            segments[-1].append(parent_sequences[k][start:])\n",
        "        else:\n",
        "            end = align_positions[i] + 1\n",
        "            segments[-1].append(parent_sequences[k][start:end])\n",
        "\n",
        "## remove gaps in sequences            \n",
        "for i in range(len(segments)):\n",
        "    for j in range(len(segments[i])):\n",
        "        segments[i][j] = \"\".join([s for s in segments[i][j] if s != \".\"])\n",
        "\n",
        "## generate all possible name        \n",
        "def generate_name(str_list):    \n",
        "    if len(str_list[0]) == 8:\n",
        "        return str_list\n",
        "    else:\n",
        "        new_list = []    \n",
        "        for s in str_list:\n",
        "            for i in ['1', '2', '3']:\n",
        "                new_list.append(s + i)\n",
        "        return generate_name(new_list)\n",
        "names = generate_name(['1', '2', '3'])\n",
        "\n",
        "## paste segments based on its name to make chimera sequences\n",
        "chimera_sequences = {}\n",
        "for name in names:\n",
        "    seq = \"\"\n",
        "    for i in range(len(name)):\n",
        "        seq += segments[int(name[i])-1][i]\n",
        "    chimera_sequences[name] = seq\n",
        "\n",
        "## save chimera sequences    \n",
        "with open(\"./output/chimera_sequences.pkl\", 'wb') as file_handle:\n",
        "    pickle.dump(chimera_sequences, file_handle)\n",
        "\n",
        "with open(\"./output/chimera_sequence.fasta\", 'w') as file_handle:\n",
        "    for key, seq in chimera_sequences.items():\n",
        "        print(\"> \" + key, sep = \"\", file = file_handle)\n",
        "        print(seq, file = file_handle)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jljq7dGxjhWk"
      },
      "source": [
        "%%bash\n",
        "\n",
        "cat ./output/query_seq.fasta \\\n",
        "    ./output/chimera_sequence.fasta \\\n",
        "    ./chimera_sequence/other_sequences.fasta > ./output/T50_sequences.fasta\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2xisHMyW5Ir"
      },
      "source": [
        "drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO-XHnUG99A2"
      },
      "source": [
        "%%bash -s \"$pfam_id\" \"$gs\"\n",
        "zip -r $1\\_results_arnold_1.zip *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ztE725zSMxc"
      },
      "source": [
        "drive.mount('./drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8Gm-XG3KryI"
      },
      "source": [
        "%%bash -s \"$pfam_id\" \"$gs\"\n",
        "mv $1\\_results_arnold_1.zip ./drive/My\\ Drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Innhxl6SpR6"
      },
      "source": [
        "drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOFJoAE7fzGT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}